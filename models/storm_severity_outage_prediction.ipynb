{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11531217,"sourceType":"datasetVersion","datasetId":7232568}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\nstorm_df = pd.read_csv(\"/kaggle/input/noaa-powout-prism-0-1-is-storm-lag/noaapowoutprism_01_Is_Storm_Lag (1).csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T15:32:27.776886Z","iopub.execute_input":"2025-04-30T15:32:27.777101Z","iopub.status.idle":"2025-04-30T15:32:35.261938Z","shell.execute_reply.started":"2025-04-30T15:32:27.777075Z","shell.execute_reply":"2025-04-30T15:32:35.261110Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Storm Prediction","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\n\n# Define features\nstorm_features = [\n    'DEATHS_INDIRECT', 'INJURIES_DIRECT', 'INJURIES_INDIRECT', 'DEATHS_DIRECT',\n    'DAMAGE_PROPERTY', 'DAMAGE_CROPS', 'customers_out', 'duration_hours',\n    'desc_word_count', 'desc_char_count',\n    'has_tornado', 'has_hail', 'has_flood', 'has_wind', 'has_tree',\n    'has_power', 'has_damage', 'has_outage', 'has_broken', 'has_blown',\n    'tmin', 'tmax', 'tavg', 'ppt'\n]\n\n# Prepare X and y\nX = storm_df[storm_features]\ny = storm_df['is_storm_lagged'].astype(int)\n\n# Scale features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T15:32:35.263594Z","iopub.execute_input":"2025-04-30T15:32:35.263850Z","iopub.status.idle":"2025-04-30T15:32:35.757731Z","shell.execute_reply.started":"2025-04-30T15:32:35.263832Z","shell.execute_reply":"2025-04-30T15:32:35.757122Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Train Random Forest model on all data with best hyperparameters obtained using optuna\nrf_model = RandomForestClassifier(\n    n_estimators=188,\n    max_depth=50,\n    min_samples_split=2,\n    min_samples_leaf=1,\n    max_features='log2',\n    bootstrap=False,\n    random_state=42,\n    n_jobs=-1\n)\nrf_model.fit(X_scaled, y)\n\n# Predict on all data\npredictions = rf_model.predict(X_scaled)\n\n# Create new DataFrame with predictions\nstorm_df_with_predictions = storm_df.copy()\nstorm_df_with_predictions['predicted_storm'] = predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T15:32:35.758503Z","iopub.execute_input":"2025-04-30T15:32:35.758794Z","iopub.status.idle":"2025-04-30T15:33:14.219289Z","shell.execute_reply.started":"2025-04-30T15:32:35.758770Z","shell.execute_reply":"2025-04-30T15:33:14.218686Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"storm_df_with_1_predictions = storm_df_with_predictions[storm_df_with_predictions['predicted_storm'] == 1].copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T15:33:14.220050Z","iopub.execute_input":"2025-04-30T15:33:14.220437Z","iopub.status.idle":"2025-04-30T15:33:14.305247Z","shell.execute_reply.started":"2025-04-30T15:33:14.220416Z","shell.execute_reply":"2025-04-30T15:33:14.304721Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Severity Prediction","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nseverity_features = [\n    # Original impact features\n    'DEATHS_INDIRECT', 'INJURIES_DIRECT', 'INJURIES_INDIRECT', 'DEATHS_DIRECT',\n    'DAMAGE_PROPERTY', 'DAMAGE_CROPS', 'customers_out', 'duration_hours',\n\n    # NLP-derived features\n    'desc_word_count', 'desc_char_count',\n    'has_tornado', 'has_hail', 'has_flood', 'has_wind', 'has_tree',\n    'has_power', 'has_damage', 'has_outage', 'has_broken', 'has_blown',\n\n    #prism features\n    'tmin', 'tmax', 'tavg', 'ppt'\n]\n\n# Features and target\nX = storm_df_with_1_predictions[severity_features]\ny = storm_df_with_1_predictions['severity_class']\n\n# Scale features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_scaled = scaler.transform(X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T15:33:14.306236Z","iopub.execute_input":"2025-04-30T15:33:14.306498Z","iopub.status.idle":"2025-04-30T15:33:14.375081Z","shell.execute_reply.started":"2025-04-30T15:33:14.306475Z","shell.execute_reply":"2025-04-30T15:33:14.374415Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Train Random Forest model on all data with best hyperparameters obtained using optuna\nrf_model = RandomForestClassifier(\n    n_estimators=201,\n    max_depth=38,\n    min_samples_split=4,\n    min_samples_leaf=1,\n    max_features='sqrt',\n    bootstrap=False,\n    random_state=42,\n    n_jobs=-1\n)\nrf_model.fit(X_scaled, y)\n\n# Predict on all data\npredictions = rf_model.predict(X_scaled)\n\n# Create new DataFrame with predictions\nstorm_data_with_severity = storm_df_with_1_predictions.copy()\nstorm_data_with_severity['severity_predicted'] = predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T15:33:14.375890Z","iopub.execute_input":"2025-04-30T15:33:14.376108Z","iopub.status.idle":"2025-04-30T15:33:34.304186Z","shell.execute_reply.started":"2025-04-30T15:33:14.376092Z","shell.execute_reply":"2025-04-30T15:33:34.303585Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# 71% --> 86.5% (after tuning)****** TOO MUCH INC, SHOULD WE USE THIS INSTEAD ??\n\n# from sklearn.preprocessing import StandardScaler, LabelEncoder\n# from xgboost import XGBClassifier\n\n# # Train XGBoost classifier on all data\n# xgb_model = XGBClassifier(\n#     n_estimators=403,\n#     max_depth=10,\n#     learning_rate=0.09065400280278058,\n#     subsample=0.933968095670629,\n#     colsample_bytree=0.5647574078202744,\n#     gamma=0.00017586655077512627,\n#     min_child_weight=2,\n#     use_label_encoder=False,\n#     eval_metric='logloss',\n#     random_state=42\n# )\n# xgb_model.fit(X_scaled, y_encoded)\n\n# # Predict on the full dataset\n# predictions_encoded = xgb_model.predict(X_scaled)\n# predictions = le.inverse_transform(predictions_encoded)\n\n# storm_data_with_severity = storm_df_with_1_predictions.copy()\n# storm_data_with_severity['severity_predicted'] = predictions\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T15:33:34.306392Z","iopub.execute_input":"2025-04-30T15:33:34.306601Z","iopub.status.idle":"2025-04-30T15:33:34.310422Z","shell.execute_reply.started":"2025-04-30T15:33:34.306584Z","shell.execute_reply":"2025-04-30T15:33:34.309673Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Create three DataFrames based on predicted severity\ndf_low = storm_data_with_severity[storm_data_with_severity['severity_predicted'] == 0]\ndf_medium = storm_data_with_severity[storm_data_with_severity['severity_predicted'] == 1]\ndf_high = storm_data_with_severity[storm_data_with_severity['severity_predicted'] == 2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T15:33:34.311045Z","iopub.execute_input":"2025-04-30T15:33:34.311351Z","iopub.status.idle":"2025-04-30T15:33:34.369154Z","shell.execute_reply.started":"2025-04-30T15:33:34.311327Z","shell.execute_reply":"2025-04-30T15:33:34.368406Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Outage Prediction","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport lightgbm as lgb\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense\n\n# Define outage features (common for all models)\noutage_features = [\n    'tmin', 'tmax', 'tavg', 'ppt',\n    'has_tornado', 'has_hail', 'has_flood', 'has_wind', 'has_tree',\n    'DAMAGE_PROPERTY', 'DAMAGE_CROPS', 'duration_hours',\n    'desc_word_count', 'desc_char_count'\n]\n\n# Derive is_outage target (assuming customers_out > 0 indicates an outage)\nfor df in [df_low, df_medium, df_high]:\n    df['is_outage'] = (df['customers_out'] > 0).astype(int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T15:33:34.370012Z","iopub.execute_input":"2025-04-30T15:33:34.370390Z","iopub.status.idle":"2025-04-30T15:33:50.941065Z","shell.execute_reply.started":"2025-04-30T15:33:34.370366Z","shell.execute_reply":"2025-04-30T15:33:50.940400Z"}},"outputs":[{"name":"stderr","text":"2025-04-30 15:33:40.281935: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746027220.471137      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746027220.525573      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/tmp/ipykernel_31/1656639219.py:21: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['is_outage'] = (df['customers_out'] > 0).astype(int)\n/tmp/ipykernel_31/1656639219.py:21: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['is_outage'] = (df['customers_out'] > 0).astype(int)\n/tmp/ipykernel_31/1656639219.py:21: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['is_outage'] = (df['customers_out'] > 0).astype(int)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Extract non-storm rows from storm_df_with_predictions\nnon_storm_data = storm_df_with_predictions[storm_df_with_predictions['predicted_storm'] == 0].copy()\nnon_storm_data['is_outage'] = 0  # No outage for non-storm events\nnon_storm_data['severity_predicted'] = 10  # Placeholder for non-storm rows","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T15:33:50.941853Z","iopub.execute_input":"2025-04-30T15:33:50.942484Z","iopub.status.idle":"2025-04-30T15:33:51.033401Z","shell.execute_reply.started":"2025-04-30T15:33:50.942462Z","shell.execute_reply":"2025-04-30T15:33:51.032816Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Randomly split non-storm rows across the three DataFrames\nnon_storm_split = np.array_split(non_storm_data.sample(frac=1, random_state=42), 3)\nnon_storm_low, non_storm_medium, non_storm_high = non_storm_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T15:33:51.034260Z","iopub.execute_input":"2025-04-30T15:33:51.034487Z","iopub.status.idle":"2025-04-30T15:33:51.276672Z","shell.execute_reply.started":"2025-04-30T15:33:51.034470Z","shell.execute_reply":"2025-04-30T15:33:51.275878Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n  return bound(*args, **kwds)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Augment DataFrames with non-storm rows\ndf_low = pd.concat([df_low, non_storm_low], ignore_index=True)\ndf_medium = pd.concat([df_medium, non_storm_medium], ignore_index=True)\ndf_high = pd.concat([df_high, non_storm_high], ignore_index=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T15:33:51.277439Z","iopub.execute_input":"2025-04-30T15:33:51.277671Z","iopub.status.idle":"2025-04-30T15:33:51.326109Z","shell.execute_reply.started":"2025-04-30T15:33:51.277647Z","shell.execute_reply":"2025-04-30T15:33:51.325317Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"### Low Severity","metadata":{}},{"cell_type":"code","source":"# 1. LightGBM for Low Severity\n\nif not df_low.empty:\n    X_low = df_low[outage_features]\n    y_low = df_low['is_outage']\n    \n    # Train-test split\n    X_train_low, X_test_low, y_train_low, y_test_low = train_test_split(\n        X_low, y_low, test_size=0.2, stratify=y_low, random_state=42\n    )\n    \n    # Feature scaling\n    scaler_low = StandardScaler()\n    X_train_low_scaled = scaler_low.fit_transform(X_train_low)\n    X_test_low_scaled = scaler_low.transform(X_test_low)\n\n    # Best parameters from Optuna Trial 32\n    best_params = {\n        'n_estimators': 319,\n        'learning_rate': 0.047847333909262976,\n        'num_leaves': 281,\n        'max_depth': 20,\n        'min_child_samples': 22,\n        'subsample': 0.527136639688917,\n        'colsample_bytree': 0.8326768083509417,\n        'random_state': 42\n    }\n\n    # Train final LightGBM model\n    final_lgb_model = lgb.LGBMClassifier(**best_params)\n    final_lgb_model.fit(X_train_low_scaled, y_train_low)\n    \n    # Predictions\n    low_predictions = final_lgb_model.predict(X_test_low_scaled)\n    \n    # Evaluation\n    print(\"Final Tuned LightGBM (Low Severity) Metrics:\")\n    print(\"Accuracy:\", accuracy_score(y_test_low, low_predictions))\n    print(\"Classification Report:\\n\", classification_report(y_test_low, low_predictions))\n    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_low, low_predictions))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T15:33:51.326918Z","iopub.execute_input":"2025-04-30T15:33:51.327128Z","iopub.status.idle":"2025-04-30T15:33:56.056038Z","shell.execute_reply.started":"2025-04-30T15:33:51.327112Z","shell.execute_reply":"2025-04-30T15:33:56.055306Z"}},"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 25768, number of negative: 27885\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004517 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2096\n[LightGBM] [Info] Number of data points in the train set: 53653, number of used features: 14\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.480271 -> initscore=-0.078955\n[LightGBM] [Info] Start training from score -0.078955\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nFinal Tuned LightGBM (Low Severity) Metrics:\nAccuracy: 0.9513195169226182\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.96      0.94      0.95      6972\n           1       0.94      0.96      0.95      6442\n\n    accuracy                           0.95     13414\n   macro avg       0.95      0.95      0.95     13414\nweighted avg       0.95      0.95      0.95     13414\n\nConfusion Matrix:\n [[6578  394]\n [ 259 6183]]\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\n# 2. XGBoost for Low Severity\nif not df_low.empty:\n    X_low = df_low[outage_features]\n    y_low = df_low['is_outage']\n    \n    # Train-test split\n    X_train_low, X_test_low, y_train_low, y_test_low = train_test_split(\n        X_low, y_low, test_size=0.2, stratify=y_low, random_state=42\n    )\n    \n    # Scale features\n    scaler_low = StandardScaler()\n    X_train_low_scaled = scaler_low.fit_transform(X_train_low)\n    X_test_low_scaled = scaler_low.transform(X_test_low)\n    \n    # Train XGBoost with best hyperparameters\n    xgb_model = XGBClassifier(\n        n_estimators=250,\n        max_depth=10,\n        learning_rate=0.06249125027083313,\n        subsample=0.5011230522934016,\n        colsample_bytree=0.7916700511130836,\n        gamma=1.0313638666099596e-05,\n        min_child_weight=1,\n        use_label_encoder=False,\n        eval_metric='logloss',\n        random_state=42\n    )\n    xgb_model.fit(X_train_low_scaled, y_train_low)\n    \n    # Predict on test set\n    low_predictions = xgb_model.predict(X_test_low_scaled)\n    \n    # Evaluate\n    print(\"XGBoost (Low Severity, Tuned) Metrics:\")\n    print(\"Accuracy:\", accuracy_score(y_test_low, low_predictions))\n    print(\"Classification Report:\\n\", classification_report(y_test_low, low_predictions))\n    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_low, low_predictions))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T15:33:56.057032Z","iopub.execute_input":"2025-04-30T15:33:56.057328Z","iopub.status.idle":"2025-04-30T15:33:58.012557Z","shell.execute_reply.started":"2025-04-30T15:33:56.057306Z","shell.execute_reply":"2025-04-30T15:33:58.012043Z"}},"outputs":[{"name":"stdout","text":"XGBoost (Low Severity, Tuned) Metrics:\nAccuracy: 0.94379007007604\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.96      0.93      0.95      6972\n           1       0.93      0.96      0.94      6442\n\n    accuracy                           0.94     13414\n   macro avg       0.94      0.94      0.94     13414\nweighted avg       0.94      0.94      0.94     13414\n\nConfusion Matrix:\n [[6498  474]\n [ 280 6162]]\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# 3. Random Forest for Low Severity\nif not df_low.empty:\n    X_low = df_low[outage_features]\n    y_low = df_low['is_outage']\n    \n    # Train-test split\n    X_train_low, X_test_low, y_train_low, y_test_low = train_test_split(\n        X_low, y_low, test_size=0.2, stratify=y_low, random_state=42\n    )\n    \n    # Scale features\n    scaler_low = StandardScaler()\n    X_train_low_scaled = scaler_low.fit_transform(X_train_low)\n    X_test_low_scaled = scaler_low.transform(X_test_low)\n    \n    # Train Random Forest\n    rf_model = RandomForestClassifier(\n        n_estimators=298,\n        max_depth=38,\n        min_samples_split=2,\n        min_samples_leaf=1,\n        max_features='sqrt',\n        bootstrap=False,\n        random_state=42,\n        n_jobs=-1\n    )\n    rf_model.fit(X_train_low_scaled, y_train_low)\n    \n    # Predict on test set\n    low_predictions = rf_model.predict(X_test_low_scaled)\n    \n    # Evaluate\n    print(\"\\nRandom Forest (Low Severity) Metrics:\")\n    print(\"Accuracy:\", accuracy_score(y_test_low, low_predictions))\n    print(\"Classification Report:\\n\", classification_report(y_test_low, low_predictions))\n    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_low, low_predictions))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T15:33:58.013096Z","iopub.execute_input":"2025-04-30T15:33:58.013441Z","iopub.status.idle":"2025-04-30T15:34:10.186862Z","shell.execute_reply.started":"2025-04-30T15:33:58.013423Z","shell.execute_reply":"2025-04-30T15:34:10.186120Z"}},"outputs":[{"name":"stdout","text":"\nRandom Forest (Low Severity) Metrics:\nAccuracy: 0.9563142984941106\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.96      0.96      0.96      6972\n           1       0.95      0.95      0.95      6442\n\n    accuracy                           0.96     13414\n   macro avg       0.96      0.96      0.96     13414\nweighted avg       0.96      0.96      0.96     13414\n\nConfusion Matrix:\n [[6677  295]\n [ 291 6151]]\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"### Medium Severity","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\nif not df_medium.empty:\n    X_medium = df_medium[outage_features]\n    y_medium = df_medium['is_outage']\n    \n    # Train-test split\n    X_train_medium, X_test_medium, y_train_medium, y_test_medium = train_test_split(\n        X_medium, y_medium, test_size=0.2, stratify=y_medium, random_state=42\n    )\n    \n    # Scale features (XGBoost can handle unscaled features, but keeping it for consistency)\n    scaler_medium = StandardScaler()\n    X_train_medium_scaled = scaler_medium.fit_transform(X_train_medium)\n    X_test_medium_scaled = scaler_medium.transform(X_test_medium)\n    \n    # Train XGBoost model\n    xgb_model = XGBClassifier(\n        n_estimators=334,\n        max_depth=10,\n        learning_rate=0.09402077273852373,\n        subsample=0.872939114369583,\n        colsample_bytree=0.7338166831128137,\n        gamma=2.050143715974591e-05,\n        min_child_weight=3,\n        use_label_encoder=False,\n        eval_metric='logloss',\n        random_state=42\n    )\n    xgb_model.fit(X_train_medium_scaled, y_train_medium)\n    \n    # Predict on test set\n    xgb_predictions = xgb_model.predict(X_test_medium_scaled)\n    \n    # Evaluate\n    print(\"\\nXGBoost (Medium Severity) Metrics:\")\n    print(\"Accuracy:\", accuracy_score(y_test_medium, xgb_predictions))\n    print(\"Classification Report:\\n\", classification_report(y_test_medium, xgb_predictions))\n    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_medium, xgb_predictions))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T15:34:10.187641Z","iopub.execute_input":"2025-04-30T15:34:10.187891Z","iopub.status.idle":"2025-04-30T15:34:11.777315Z","shell.execute_reply.started":"2025-04-30T15:34:10.187867Z","shell.execute_reply":"2025-04-30T15:34:11.776608Z"}},"outputs":[{"name":"stdout","text":"\nXGBoost (Medium Severity) Metrics:\nAccuracy: 0.9276118830122054\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.95      0.91      0.93      6958\n           1       0.90      0.95      0.92      6069\n\n    accuracy                           0.93     13027\n   macro avg       0.93      0.93      0.93     13027\nweighted avg       0.93      0.93      0.93     13027\n\nConfusion Matrix:\n [[6345  613]\n [ 330 5739]]\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# 2. Random Forest for Medium Severity\n\nif not df_medium.empty:\n    X_medium = df_medium[outage_features]\n    y_medium = df_medium['is_outage']\n    \n    # Train-test split\n    X_train_medium, X_test_medium, y_train_medium, y_test_medium = train_test_split(\n        X_medium, y_medium, test_size=0.2, stratify=y_medium, random_state=42\n    )\n    \n    # Scale features\n    scaler_medium = StandardScaler()\n    X_train_medium_scaled = scaler_medium.fit_transform(X_train_medium)\n    X_test_medium_scaled = scaler_medium.transform(X_test_medium)\n    \n    # Train Random Forest\n    rf_model = RandomForestClassifier(\n        n_estimators=191,\n        max_depth=28,\n        min_samples_split=6,\n        min_samples_leaf=2,\n        max_features='log2',\n        bootstrap=False,\n        random_state=42,\n        n_jobs=-1\n    )\n    rf_model.fit(X_train_medium_scaled, y_train_medium)\n    \n    # Predict on test set\n    medium_predictions = rf_model.predict(X_test_medium_scaled)\n    \n    # Evaluate\n    print(\"\\nRandom Forest (Medium Severity) Metrics:\")\n    print(\"Accuracy:\", accuracy_score(y_test_medium, medium_predictions))\n    print(\"Classification Report:\\n\", classification_report(y_test_medium, medium_predictions))\n    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_medium, medium_predictions))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T15:34:11.778539Z","iopub.execute_input":"2025-04-30T15:34:11.778872Z","iopub.status.idle":"2025-04-30T15:34:19.132311Z","shell.execute_reply.started":"2025-04-30T15:34:11.778833Z","shell.execute_reply":"2025-04-30T15:34:19.131541Z"}},"outputs":[{"name":"stdout","text":"\nRandom Forest (Medium Severity) Metrics:\nAccuracy: 0.936132647578107\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.95      0.93      0.94      6958\n           1       0.92      0.94      0.93      6069\n\n    accuracy                           0.94     13027\n   macro avg       0.94      0.94      0.94     13027\nweighted avg       0.94      0.94      0.94     13027\n\nConfusion Matrix:\n [[6460  498]\n [ 334 5735]]\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"### High Severity","metadata":{}},{"cell_type":"code","source":"# FNN for High Severity\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nif not df_high.empty:\n    X_high = df_high[outage_features]\n    y_high = df_high['is_outage']\n\n    # Train-test split\n    X_train_high, X_test_high, y_train_high, y_test_high = train_test_split(\n        X_high, y_high, test_size=0.2, stratify=y_high, random_state=42\n    )\n\n    # Scale features\n    scaler_high = StandardScaler()\n    X_train_high_scaled = scaler_high.fit_transform(X_train_high)\n    X_test_high_scaled = scaler_high.transform(X_test_high)\n\n    # Optimized hyperparameters from Optuna\n    n_units = [80, 112, 96]\n    dropout_rate = 0.1677425146431672\n    learning_rate = 0.001715383587455835\n    batch_size = 64\n\n    # Build optimized model\n    model = Sequential()\n    model.add(Dense(n_units[0], activation='relu', input_shape=(len(outage_features),)))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(n_units[1], activation='relu'))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(n_units[2], activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n\n    model.fit(\n        X_train_high_scaled, y_train_high,\n        epochs=20,\n        batch_size=batch_size,\n        validation_split=0.2,\n        verbose=0\n    )\n\n    # Predict on test set\n    high_predictions = (model.predict(X_test_high_scaled) > 0.5).astype(int).flatten()\n\n    # Evaluation\n    print(\"\\nOptimized FNN (High Severity) Metrics:\")\n    print(\"Accuracy:\", accuracy_score(y_test_high, high_predictions))\n    print(\"Classification Report:\\n\", classification_report(y_test_high, high_predictions))\n    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_high, high_predictions))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T15:34:19.133155Z","iopub.execute_input":"2025-04-30T15:34:19.133431Z","iopub.status.idle":"2025-04-30T15:34:49.382733Z","shell.execute_reply.started":"2025-04-30T15:34:19.133407Z","shell.execute_reply":"2025-04-30T15:34:49.382032Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\nI0000 00:00:1746027260.375010      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1746027260.375616      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1746027263.749902     157 service.cc:148] XLA service 0x781d9c005a30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1746027263.750482     157 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1746027263.750502     157 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1746027264.038422     157 cuda_dnn.cc:529] Loaded cuDNN version 90300\nI0000 00:00:1746027265.644408     157 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n\nOptimized FNN (High Severity) Metrics:\nAccuracy: 0.8710087489718089\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.91      0.83      0.87      6958\n           1       0.83      0.92      0.87      6415\n\n    accuracy                           0.87     13373\n   macro avg       0.87      0.87      0.87     13373\nweighted avg       0.87      0.87      0.87     13373\n\nConfusion Matrix:\n [[5776 1182]\n [ 543 5872]]\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# 2. Random Forest for High Severity\nif not df_high.empty:\n    X_medium = df_high[outage_features]\n    y_medium = df_high['is_outage']\n    \n    # Train-test split\n    X_train_medium, X_test_medium, y_train_medium, y_test_medium = train_test_split(\n        X_medium, y_medium, test_size=0.2, stratify=y_medium, random_state=42\n    )\n    \n    # Scale features\n    scaler_medium = StandardScaler()\n    X_train_medium_scaled = scaler_medium.fit_transform(X_train_medium)\n    X_test_medium_scaled = scaler_medium.transform(X_test_medium)\n\n    # Train Random Forest\n    rf_model = RandomForestClassifier(\n        n_estimators=225,\n        max_depth=47,\n        min_samples_split=2,\n        min_samples_leaf=1,\n        max_features='sqrt',\n        bootstrap=False,\n        random_state=42,\n        n_jobs=-1\n    )\n    rf_model.fit(X_train_medium_scaled, y_train_medium)\n    \n    # Predict on test set\n    medium_predictions = rf_model.predict(X_test_medium_scaled)\n    \n    # Evaluate\n    print(\"\\nRandom Forest (High Severity) Metrics:\")\n    print(\"Accuracy:\", accuracy_score(y_test_medium, medium_predictions))\n    print(\"Classification Report:\\n\", classification_report(y_test_medium, medium_predictions))\n    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_medium, medium_predictions))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T15:34:49.383482Z","iopub.execute_input":"2025-04-30T15:34:49.383758Z","iopub.status.idle":"2025-04-30T15:34:58.806275Z","shell.execute_reply.started":"2025-04-30T15:34:49.383740Z","shell.execute_reply":"2025-04-30T15:34:58.805429Z"}},"outputs":[{"name":"stdout","text":"\nRandom Forest (High Severity) Metrics:\nAccuracy: 0.9404770806849623\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.95      0.93      0.94      6958\n           1       0.93      0.95      0.94      6415\n\n    accuracy                           0.94     13373\n   macro avg       0.94      0.94      0.94     13373\nweighted avg       0.94      0.94      0.94     13373\n\nConfusion Matrix:\n [[6467  491]\n [ 305 6110]]\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# 3. XGBoost for High Severity\nif not df_high.empty:\n    X_high = df_high[outage_features]\n    y_high = df_high['is_outage']\n    \n    # Train-test split\n    X_train_high, X_test_high, y_train_high, y_test_high = train_test_split(\n        X_high, y_high, test_size=0.2, stratify=y_high, random_state=42\n    )\n    \n    # Scale features\n    scaler_high = StandardScaler()\n    X_train_high_scaled = scaler_high.fit_transform(X_train_high)\n    X_test_high_scaled = scaler_high.transform(X_test_high)\n\n    xgb_model = XGBClassifier(\n        n_estimators = 422, \n        max_depth = 10, \n        learning_rate = 0.06603920004888718, \n        subsample = 0.9760195263062684, \n        colsample_bytree = 0.6502775826898155, \n        gamma = 1.0450200674334978e-05,\n        min_child_weight = 3,\n        use_label_encoder=False,\n        eval_metric='logloss',\n        random_state=42\n    )\n    xgb_model.fit(X_train_high_scaled, y_train_high)\n    \n    # Predict on test set\n    high_predictions = xgb_model.predict(X_test_high_scaled)\n    \n    # Evaluate\n    print(\"\\nRandom Forest (High Severity) Metrics:\")\n    print(\"Accuracy:\", accuracy_score(y_test_high, high_predictions))\n    print(\"Classification Report:\\n\", classification_report(y_test_high, high_predictions))\n    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_high, high_predictions))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T15:34:58.807179Z","iopub.execute_input":"2025-04-30T15:34:58.807486Z","iopub.status.idle":"2025-04-30T15:35:01.738610Z","shell.execute_reply.started":"2025-04-30T15:34:58.807468Z","shell.execute_reply":"2025-04-30T15:35:01.738077Z"}},"outputs":[{"name":"stdout","text":"\nRandom Forest (High Severity) Metrics:\nAccuracy: 0.933597547296792\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.96      0.91      0.93      6958\n           1       0.91      0.96      0.93      6415\n\n    accuracy                           0.93     13373\n   macro avg       0.93      0.93      0.93     13373\nweighted avg       0.93      0.93      0.93     13373\n\nConfusion Matrix:\n [[6342  616]\n [ 272 6143]]\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import optuna\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import f1_score\nimport numpy as np\n\ndef objective(trial):\n    # Hyperparameter search space\n    n_units1 = trial.suggest_int(\"n_units1\", 32, 128)\n    n_units2 = trial.suggest_int(\"n_units2\", 32, 128)\n    n_units3 = trial.suggest_int(\"n_units3\", 32, 128)\n    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n\n    # Data split and scaling\n    X_high = df_high[outage_features]\n    y_high = df_high['is_outage']\n    X_train, X_val, y_train, y_val = train_test_split(\n        X_high, y_high, test_size=0.2, stratify=y_high, random_state=42\n    )\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_val_scaled = scaler.transform(X_val)\n\n    # Model definition\n    model = Sequential()\n    model.add(Dense(n_units1, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(n_units2, activation='relu'))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(n_units3, activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n\n    # Train the model\n    history = model.fit(\n        X_train_scaled, y_train,\n        validation_data=(X_val_scaled, y_val),\n        epochs=20,\n        batch_size=batch_size,\n        verbose=0\n    )\n\n    # Predict and return validation F1 score\n    preds = (model.predict(X_val_scaled) > 0.5).astype(int)\n    return f1_score(y_val, preds)\n\n# Only run tuning if data is available\nif not df_high.empty:\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=30)\n\n    print(\"Best trial:\")\n    print(study.best_trial)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T15:35:27.862546Z","iopub.execute_input":"2025-04-30T15:35:27.862774Z","iopub.status.idle":"2025-04-30T15:58:30.371096Z","shell.execute_reply.started":"2025-04-30T15:35:27.862757Z","shell.execute_reply":"2025-04-30T15:58:30.370327Z"}},"outputs":[{"name":"stderr","text":"[I 2025-04-30 15:35:27,868] A new study created in memory with name: no-name-faa9a608-00f7-454a-84a3-2eabc628766d\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-30 15:35:48,274] Trial 0 finished with value: 0.8596759400794864 and parameters: {'n_units1': 70, 'n_units2': 71, 'n_units3': 72, 'dropout_rate': 0.34659494980215777, 'learning_rate': 0.008275497099703811, 'batch_size': 128}. Best is trial 0 with value: 0.8596759400794864.\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-30 15:36:08,222] Trial 1 finished with value: 0.8652492777242758 and parameters: {'n_units1': 34, 'n_units2': 46, 'n_units3': 103, 'dropout_rate': 0.2831578771610723, 'learning_rate': 0.004415347921959894, 'batch_size': 128}. Best is trial 1 with value: 0.8652492777242758.\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-30 15:36:41,135] Trial 2 finished with value: 0.861039551447189 and parameters: {'n_units1': 118, 'n_units2': 88, 'n_units3': 108, 'dropout_rate': 0.44491363972796316, 'learning_rate': 0.0081944858322549, 'batch_size': 64}. Best is trial 1 with value: 0.8652492777242758.\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-30 15:37:36,960] Trial 3 finished with value: 0.8647364513734225 and parameters: {'n_units1': 38, 'n_units2': 123, 'n_units3': 49, 'dropout_rate': 0.3375981576824284, 'learning_rate': 0.00027187890985022714, 'batch_size': 32}. Best is trial 1 with value: 0.8652492777242758.\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-30 15:38:08,823] Trial 4 finished with value: 0.8655136424630983 and parameters: {'n_units1': 54, 'n_units2': 89, 'n_units3': 103, 'dropout_rate': 0.3630594293430367, 'learning_rate': 0.0013693394861851338, 'batch_size': 64}. Best is trial 4 with value: 0.8655136424630983.\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-30 15:38:29,397] Trial 5 finished with value: 0.8653372171185302 and parameters: {'n_units1': 80, 'n_units2': 126, 'n_units3': 32, 'dropout_rate': 0.29007132484273, 'learning_rate': 0.009049641441804146, 'batch_size': 128}. Best is trial 4 with value: 0.8655136424630983.\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-30 15:39:01,335] Trial 6 finished with value: 0.8658830620855936 and parameters: {'n_units1': 109, 'n_units2': 41, 'n_units3': 48, 'dropout_rate': 0.3361012151740133, 'learning_rate': 0.0021003190862670194, 'batch_size': 64}. Best is trial 6 with value: 0.8658830620855936.\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-30 15:39:22,011] Trial 7 finished with value: 0.8656205845169925 and parameters: {'n_units1': 55, 'n_units2': 93, 'n_units3': 66, 'dropout_rate': 0.4743395678477097, 'learning_rate': 0.004034183340508868, 'batch_size': 128}. Best is trial 6 with value: 0.8658830620855936.\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-30 15:40:18,892] Trial 8 finished with value: 0.8675586289837643 and parameters: {'n_units1': 123, 'n_units2': 112, 'n_units3': 104, 'dropout_rate': 0.3083909702232255, 'learning_rate': 0.00029822152971411734, 'batch_size': 32}. Best is trial 8 with value: 0.8675586289837643.\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-30 15:41:14,797] Trial 9 finished with value: 0.8642792657237437 and parameters: {'n_units1': 36, 'n_units2': 67, 'n_units3': 57, 'dropout_rate': 0.12668240832562694, 'learning_rate': 0.00017653155077661413, 'batch_size': 32}. Best is trial 8 with value: 0.8675586289837643.\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-30 15:42:11,620] Trial 10 finished with value: 0.871415770609319 and parameters: {'n_units1': 100, 'n_units2': 108, 'n_units3': 90, 'dropout_rate': 0.1957657859909165, 'learning_rate': 0.0004670703408923066, 'batch_size': 32}. Best is trial 10 with value: 0.871415770609319.\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-30 15:43:08,144] Trial 11 finished with value: 0.8691361773189618 and parameters: {'n_units1': 98, 'n_units2': 107, 'n_units3': 126, 'dropout_rate': 0.18323203518084893, 'learning_rate': 0.0004888958648003947, 'batch_size': 32}. Best is trial 10 with value: 0.871415770609319.\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-30 15:44:04,298] Trial 12 finished with value: 0.8702440503259058 and parameters: {'n_units1': 99, 'n_units2': 105, 'n_units3': 124, 'dropout_rate': 0.17067924634481832, 'learning_rate': 0.0005515788791710603, 'batch_size': 32}. Best is trial 10 with value: 0.871415770609319.\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-30 15:45:01,450] Trial 13 finished with value: 0.8718334210328497 and parameters: {'n_units1': 94, 'n_units2': 106, 'n_units3': 86, 'dropout_rate': 0.19712912235771374, 'learning_rate': 0.0007629952112710857, 'batch_size': 32}. Best is trial 13 with value: 0.8718334210328497.\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-30 15:45:57,671] Trial 14 finished with value: 0.8717413972888425 and parameters: {'n_units1': 83, 'n_units2': 103, 'n_units3': 86, 'dropout_rate': 0.21752507178514477, 'learning_rate': 0.0007969693829573891, 'batch_size': 32}. Best is trial 13 with value: 0.8718334210328497.\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-30 15:46:53,792] Trial 15 finished with value: 0.8727218404541379 and parameters: {'n_units1': 83, 'n_units2': 78, 'n_units3': 86, 'dropout_rate': 0.2330445609542523, 'learning_rate': 0.0009885204500718323, 'batch_size': 32}. Best is trial 15 with value: 0.8727218404541379.\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-30 15:47:50,960] Trial 16 finished with value: 0.8697936210131333 and parameters: {'n_units1': 86, 'n_units2': 69, 'n_units3': 76, 'dropout_rate': 0.24396695750069985, 'learning_rate': 0.0015088539059063394, 'batch_size': 32}. Best is trial 15 with value: 0.8727218404541379.\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-30 15:48:47,147] Trial 17 finished with value: 0.8628275396050755 and parameters: {'n_units1': 72, 'n_units2': 55, 'n_units3': 91, 'dropout_rate': 0.10427390900512187, 'learning_rate': 0.00010896785160007207, 'batch_size': 32}. Best is trial 15 with value: 0.8727218404541379.\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-30 15:49:43,334] Trial 18 finished with value: 0.8688350983358547 and parameters: {'n_units1': 91, 'n_units2': 83, 'n_units3': 82, 'dropout_rate': 0.246560249151753, 'learning_rate': 0.0009627570648746236, 'batch_size': 32}. Best is trial 15 with value: 0.8727218404541379.\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-30 15:50:16,521] Trial 19 finished with value: 0.8689149779406267 and parameters: {'n_units1': 67, 'n_units2': 32, 'n_units3': 62, 'dropout_rate': 0.1472897787426269, 'learning_rate': 0.002442957019493165, 'batch_size': 64}. Best is trial 15 with value: 0.8727218404541379.\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-30 15:51:12,840] Trial 20 finished with value: 0.8697335119845169 and parameters: {'n_units1': 111, 'n_units2': 74, 'n_units3': 93, 'dropout_rate': 0.2427838591063986, 'learning_rate': 0.0007323074881518738, 'batch_size': 32}. Best is trial 15 with value: 0.8727218404541379.\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-30 15:52:08,902] Trial 21 finished with value: 0.8696565171741414 and parameters: {'n_units1': 83, 'n_units2': 98, 'n_units3': 85, 'dropout_rate': 0.20768105220252392, 'learning_rate': 0.0009752595074138901, 'batch_size': 32}. Best is trial 15 with value: 0.8727218404541379.\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-30 15:53:06,070] Trial 22 finished with value: 0.8735580858822654 and parameters: {'n_units1': 91, 'n_units2': 120, 'n_units3': 113, 'dropout_rate': 0.22266787570310717, 'learning_rate': 0.0007229270325478747, 'batch_size': 32}. Best is trial 22 with value: 0.8735580858822654.\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-30 15:54:02,524] Trial 23 finished with value: 0.8728070175438597 and parameters: {'n_units1': 94, 'n_units2': 118, 'n_units3': 113, 'dropout_rate': 0.1553930303262647, 'learning_rate': 0.0014735418312566944, 'batch_size': 32}. Best is trial 22 with value: 0.8735580858822654.\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-30 15:54:59,023] Trial 24 finished with value: 0.8739670959577162 and parameters: {'n_units1': 107, 'n_units2': 117, 'n_units3': 117, 'dropout_rate': 0.1569222852994605, 'learning_rate': 0.0015696670307294182, 'batch_size': 32}. Best is trial 24 with value: 0.8739670959577162.\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-30 15:55:56,119] Trial 25 finished with value: 0.8743365478059356 and parameters: {'n_units1': 108, 'n_units2': 117, 'n_units3': 116, 'dropout_rate': 0.1552703524420285, 'learning_rate': 0.0027760354434739442, 'batch_size': 32}. Best is trial 25 with value: 0.8743365478059356.\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-30 15:56:53,662] Trial 26 finished with value: 0.8753085496297404 and parameters: {'n_units1': 110, 'n_units2': 117, 'n_units3': 116, 'dropout_rate': 0.10607662795002712, 'learning_rate': 0.003063903238066634, 'batch_size': 32}. Best is trial 26 with value: 0.8753085496297404.\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-30 15:57:49,964] Trial 27 finished with value: 0.8726550430368571 and parameters: {'n_units1': 108, 'n_units2': 117, 'n_units3': 119, 'dropout_rate': 0.10569100838548284, 'learning_rate': 0.0036083849004718937, 'batch_size': 32}. Best is trial 26 with value: 0.8753085496297404.\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-30 15:58:10,196] Trial 28 finished with value: 0.874450341167551 and parameters: {'n_units1': 127, 'n_units2': 113, 'n_units3': 118, 'dropout_rate': 0.13442637898797585, 'learning_rate': 0.0027934241225777943, 'batch_size': 128}. Best is trial 26 with value: 0.8753085496297404.\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-30 15:58:30,367] Trial 29 finished with value: 0.8733958724202627 and parameters: {'n_units1': 127, 'n_units2': 97, 'n_units3': 128, 'dropout_rate': 0.12977025968024897, 'learning_rate': 0.005777122511596615, 'batch_size': 128}. Best is trial 26 with value: 0.8753085496297404.\n","output_type":"stream"},{"name":"stdout","text":"Best trial:\nFrozenTrial(number=26, state=1, values=[0.8753085496297404], datetime_start=datetime.datetime(2025, 4, 30, 15, 55, 56, 120171), datetime_complete=datetime.datetime(2025, 4, 30, 15, 56, 53, 661818), params={'n_units1': 110, 'n_units2': 117, 'n_units3': 116, 'dropout_rate': 0.10607662795002712, 'learning_rate': 0.003063903238066634, 'batch_size': 32}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'n_units1': IntDistribution(high=128, log=False, low=32, step=1), 'n_units2': IntDistribution(high=128, log=False, low=32, step=1), 'n_units3': IntDistribution(high=128, log=False, low=32, step=1), 'dropout_rate': FloatDistribution(high=0.5, log=False, low=0.1, step=None), 'learning_rate': FloatDistribution(high=0.01, log=True, low=0.0001, step=None), 'batch_size': CategoricalDistribution(choices=(32, 64, 128))}, trial_id=26, value=None)\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}